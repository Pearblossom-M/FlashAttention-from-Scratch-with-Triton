# ä»é›¶å®ç° FlashAttentionï¼ˆPhase 4ï¼‰ï¼šåå‘ä¼ æ’­ â€”â€” Recomputation çš„æ™ºæ…§

ç›®æ ‡ï¼š

- æŒæ¡ FlashAttention backward çš„æ•°å­¦åŸç†å’Œ recomputation ç­–ç•¥
- å®ç° dQ å’Œ dKV ä¸¤ä¸ª backward kernel
- å°† FlashAttention é›†æˆåˆ° PyTorch çš„ autograd ç³»ç»Ÿä¸­

è¯´æ˜ï¼š

- æœ¬ç« ä¸“æ³¨äº**ç®—æ³•æ­£ç¡®æ€§**ï¼Œæ€§èƒ½ä¼˜åŒ–ç•™åˆ° Phase 5

------

## 4.1 Backward çš„æ•°å­¦åŸºç¡€

**ç›®æ ‡**ï¼šç†è§£ dQ, dK, dV çš„æ¢¯åº¦å…¬å¼

### 4.1.1 çŸ©é˜µæ¢¯åº¦åŸºç¡€

å¦‚æœä½ è¿˜æ²¡æœ‰çŸ©é˜µå¾®ç§¯åˆ†åŸºç¡€ï¼Œé‚£ä¹ˆéå¸¸å»ºè®®å…ˆçœ‹å®Œè¿™ä¸€èŠ‚ï¼ˆç®€å•ä½†å¤Ÿç”¨ï¼‰ã€‚å¦‚æœä½ å·²ç»å­¦è¿‡çŸ©é˜µæ¢¯åº¦ï¼Œåˆ™å¯ä»¥ç›´æ¥è·³è¿‡ã€‚

åœ¨æ¨å¯¼ Attention çš„æ¢¯åº¦æ—¶ï¼Œæˆ‘ä»¬ä¼šç”¨åˆ°ä»¥ä¸‹å‡ ä¸ª**åŸºç¡€å…¬å¼**ï¼š

**å…¬å¼ 1ï¼šçŸ©é˜µä¹˜æ³•çš„æ¢¯åº¦**

å¦‚æœ `C = A @ B`ï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰ï¼Œå¹¶ä¸”å·²çŸ¥ `dC`ï¼ˆå³æŸå¤±å¯¹ C çš„æ¢¯åº¦ï¼š$\frac{\partial L}{\partial C}$ï¼‰ï¼Œé‚£ä¹ˆï¼š

```python
dA = dC @ B^T
dB = A^T @ dC
```

* `dA`ã€`dB` åˆ†åˆ«è¡¨ç¤ºæŸå¤±å¯¹ Aã€B çš„æ¢¯åº¦ï¼š$\frac{\partial L}{\partial A}$ å’Œ $\frac{\partial L}{\partial B}$

è¿™ä¸ªå…¬å¼å¯ä»¥ç”¨å¾®åˆ†æ³• + Frobenius å†…ç§¯æ¨å¯¼ï¼Œä½†è®°ä½ç»“è®ºå°±å¤Ÿç”¨äº†ã€‚å…¶ä¸­ï¼š

- `A` çš„å½¢çŠ¶ï¼š`[M, K]`ï¼Œ`B` çš„å½¢çŠ¶ï¼š`[K, N]`
- `C` çš„å½¢çŠ¶ï¼š`[M, N]`ï¼Œ`dC` çš„å½¢çŠ¶ï¼š`[M, N]`
- `dA = dC @ B^T` çš„å½¢çŠ¶ï¼š`[M, N] @ [N, K] = [M, K]`ï¼Œä¸ `A` å½¢çŠ¶ç›¸åŒâœ…
- `dB = A^T @ dC` çš„å½¢çŠ¶ï¼š`[K, M] @ [M, N] = [K, N]`ï¼Œä¸ `B` å½¢çŠ¶ç›¸åŒâœ…

**å…¬å¼ 2ï¼šelement-wise è¿ç®—çš„æ¢¯åº¦**

å¦‚æœ `C = A âŠ™ B`ï¼ˆelement-wise ä¹˜æ³•ï¼Œå³å¯¹åº”ä½ç½®ç›¸ä¹˜ï¼‰ï¼Œé‚£ä¹ˆï¼š

```python
dA = dC âŠ™ B # A, B, C shape: [M, N]
dB = dC âŠ™ A
```

ä¸åŒä½ç½®ä¹‹é—´å®Œå…¨è§£è€¦ï¼Œæ¯ä¸ª `C_ij` åªä¸ `A_ij` å’Œ `B_ij` æœ‰å…³ï¼Œæ‰€ä»¥ `dA_ij` =  $\frac{\partial L}{\partial A_{ij}}$ = $\frac{\partial L}{\partial C_{ij}}$$\frac{\partial C_{ij}}{\partial A_{ij}}$ = `dC_ij * B_ij`ã€‚

**å…¬å¼ 3ï¼šç¼©æ”¾çš„æ¢¯åº¦**

å¦‚æœ `B = Î±A`ï¼ˆæ ‡é‡ä¹˜æ³•ï¼ŒÎ± ä¸ºå¸¸æ•°æ ‡é‡ï¼‰ï¼Œé‚£ä¹ˆï¼š

```python
dA = dB * Î±
```

* è¿™æ˜¯ä¸€ä¸ªæ ‡é‡çº¿æ€§å‡½æ•°ï¼Œå› æ­¤$\frac{\partial B_{ij}}{\partial A_{ij}}$ = Î±
* `dA` = $\frac{\partial L}{\partial A}$ = $\frac{\partial L}{\partial B} $$\frac{\partial B}{\partial A}$ = `dB * Î±`

**å…¬å¼ 4ï¼šreduction**

å¦‚æœ $B_i = \sum_j A_{ij}$ ï¼ˆå¯¹ j ç»´åº¦åš reductionï¼‰ï¼Œ é‚£ä¹ˆï¼š

```python
dA_ij = dB_i # A shape: [M, N], B shape: [M,]
```

$\frac{\partial B_{i}}{\partial A_{ij}}$ = 1ï¼Œæ‰€ä»¥ï¼š`dA_ij` = $\frac{\partial L}{\partial A_{ij}}$ = $\frac{\partial L}{\partial B_{i}}$ $\frac{\partial B_{i}}{\partial A_{ij}}$ = `dB_i`

**å…¬å¼ 5ï¼šbroadcast**

å¦‚æœ $B_{ij} = A_i$ï¼Œ é‚£ä¹ˆï¼š

```python
dA_i = sum_j dB_ij # A shape: [M,], B shape: [M, N]
```

$\frac{\partial B_{ij}}{\partial A_{i}}$ = 1ï¼Œæ‰€ä»¥ï¼š`dA_j` = $\sum_j \frac{\partial L}{\partial A_{i}}$ = $\sum_j \frac{\partial L}{\partial B_{ij}}$ $\frac{\partial B_{ij}}{\partial A_{i}}$ = $\sum_j$ `dB_ij`

> ä¸€ä¸ªå˜é‡åœ¨ forward ä¸­è¢«ç”¨å‡ æ¬¡ï¼Œå®ƒåœ¨ backward ä¸­çš„æ¢¯åº¦å°±è¦ç´¯åŠ å‡ æ¬¡ã€‚æˆ–è€…è¯´ï¼šå½“ä¸€ä¸ªå˜é‡é€šè¿‡å¤šæ¡è·¯å¾„å½±å“æŸå¤±æ—¶ï¼Œæ¢¯åº¦å°±æ˜¯è¿™äº›è·¯å¾„è´¡çŒ®çš„å’Œã€‚

### 4.1.2 æ ‡å‡† Attention çš„åå‘ä¼ æ’­

å…ˆå›é¡¾ä¸€ä¸‹æ ‡å‡† Attention çš„ forward å…¬å¼ï¼š

```python
S = Q @ K^T / sqrt(D) 
P = softmax(S)  # row-wise
O = P @ V
```

æ ‡å‡† Attention çš„æ¢¯åº¦å…¬å¼ï¼š

```python
dV = P^T @ dO
dP = dO @ V^T
delta = row_sum(dO âŠ™ O) # ä¼˜åŒ–ï¼šé¿å…æ˜¾å¼è®¡ç®— P âŠ™ dP
dS = P âŠ™ (dP - delta[:, None])
dQ = dS @ K / sqrt(D)
dK = dS^T @ Q / sqrt(D)
```

> è¯¦ç»†çš„æ¨å¯¼è¿‡ç¨‹æ”¾åœ¨äº†æœ¬ç« æœ€åçš„**é™„å½•**ä¸­ã€‚

å¯ä»¥å‘ç°ï¼š

- æ¢¯åº¦æ±‚è§£ä¾èµ– `P` çŸ©é˜µ
- æ ‡å‡†å®ç°ä¸­ï¼Œ`P` åœ¨ forward æ—¶ä¿å­˜äº†ä¸‹æ¥ï¼Œä½†åœ¨ FlashAttention ä¸­ï¼Œ`P` å¹¶æœªä¿å­˜

é‚£å¦‚ä½•åœ¨æ²¡æœ‰ `P` çš„æƒ…å†µä¸‹è®¡ç®—æ¢¯åº¦å‘¢ï¼Ÿè¿™å°±è¦ç”¨åˆ°æˆ‘ä»¬åœ¨ Phase 3 çš„ 3.3 èŠ‚ä¸­æåˆ°çš„ **Recomputation** äº†ã€‚



------

## 4.2 Recomputation ç­–ç•¥

**ç›®æ ‡**ï¼šç†è§£å¦‚ä½•ç”¨ `Q`ã€`K`ã€`LSE` é‡å»º `P`

### 4.2.1 ä» LSE é‡å»º P

å›é¡¾ forward æ—¶çš„è®¡ç®—ï¼š

```python
# è®¡ç®— score çŸ©é˜µ
S_ij = (Q_i Â· K_j) / sqrt(D)
# è®¡ç®—ç»Ÿè®¡é‡
m_i = max_j(S_ij)
l_i = sum_j(exp(S_ij - m_i))
# è®¡ç®—æ¦‚ç‡çŸ©é˜µ
P_ij = exp(S_ij - m_i) / l_i
```

é—®é¢˜æè¿°ï¼š

* å·²çŸ¥ï¼š`Q`ã€`K`ã€`LSE` ï¼Œå…¶ä¸­ `LSE_i = m_i + log(l_i)`ï¼Œæ±‚ï¼š`P_ij`

**é‡å»ºå…¬å¼ï¼š**

* **æ­¥éª¤ä¸€**ï¼šé‡ç®— `S_ij = (Q_i Â· K_j) / sqrt(D)`
* **æ­¥éª¤äºŒ**ï¼šç”¨ `S_ij` å’Œ `LSE` è¿˜åŸ `P_ij`ï¼Œ`P_ij = exp(S_ij - LSE_i)`

è¯æ˜ï¼š

```python
LSE_i = m_i + log(l_i)
P_ij = exp(S_ij - m_i) / l_i
= exp(S_ij - m_i) / exp(log(l_i))
= exp(S_ij - m_i - log(l_i))
= exp(S_ij - (m_i + log(l_i)))
= exp(S_ij - LSE_i)
```

> è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬å¯ä»¥åœ¨ Phase 3 çš„ 3.3 èŠ‚å­˜ä¸€ä¸ªåˆå¹¶çš„å€¼ $LSE$ ï¼Œè€Œä¸ç”¨åˆ†åˆ«å­˜ $m$ å’Œ $l$ çš„åŸå› 

é€šè¿‡ Recomputation é‡å»º `P` åï¼Œå°±å¯ä»¥ç”¨äºè®¡ç®—æ¢¯åº¦äº†ã€‚

### 4.2.2 ä¸ºä»€ä¹ˆä½¿ç”¨ Recomputationï¼Ÿ

è¿™é‡Œå¤§å®¶å¯èƒ½ä¼šæœ‰ä¸€ä¸ªç–‘é—®ï¼Œä¸ºä»€ä¹ˆä¸€å®šè¦**ä½¿ç”¨ Recomputation é‡å»º `P`** å‘¢ï¼Ÿ

ä¸»è¦åŸå› æœ‰ä¸¤ç‚¹ï¼š

1. FlashAttention çš„æ ¸å¿ƒåŸç†å°±æ˜¯ä¸å­˜å‚¨å®Œæ•´çš„ `P` çŸ©é˜µï¼Œä»è€Œå‡å°‘æ˜¾å­˜å ç”¨ï¼Œé™ä½è®¿å­˜å¼€é”€ï¼Œæé«˜è®¡ç®—ååé‡
2. ä½¿ç”¨ Recomputation é‡å»º `P` å®é™…ä¸Š**æ›´å¿«**ï¼

è¿™é‡Œå¯¹æ¯”ä¸€ä¸‹ä¸¤ç§ç­–ç•¥çš„è®¿å­˜å¼€é”€ï¼š

|          | ç­–ç•¥ A â€”â€” å­˜ P                 | ç­–ç•¥ B â€”â€” Recomputation                                  |
| -------- | ------------------------------ | -------------------------------------------------------- |
| Forward  | å†™ `P` åˆ° HBM â€”â€” $O(N^2)$ è®¿å­˜ | å†™ `LSE` åˆ° HBM â€”â€” $O(N)$ è®¿å­˜                           |
| Backward | ä» HBM è¯» `P` â€”â€” $O(N^2)$ è®¿å­˜ | ä» HBM è¯» `LSE`ï¼Œé‡ç®— `P` â€”â€” $O(N)$ è®¿å­˜ + $O(N^2)$ è®¡ç®— |
| æ€»è®¿å­˜   | $O(N^2)$                       | $O(N)$                                                   |

ç”±äº GPU çš„è®¡ç®—é€Ÿåº¦ >> è®¿å­˜é€Ÿåº¦ï¼Œä½¿å¾— $O(N^2)$ çš„è®¡ç®—ä»£ä»· < $O(N^2)$ çš„è®¿å­˜ä»£ä»·ï¼Œæ‰€ä»¥ **Recomputation åè€Œæ›´å¿«**ï¼

> Recomputation çš„æœ¬è´¨å°±æ˜¯ç”¨è®¡ç®—æ¢ç©ºé—´ï¼Œå¹¸è¿çš„æ˜¯ï¼Œè¿™é‡Œè®¡ç®—å¾ˆå¿«ï¼Œæ‰€ä»¥å®é™…ä¸Šä¸ä»…æ¢äº†ç©ºé—´ï¼Œä¹Ÿæ¢åˆ°äº†æ—¶é—´ â€”â€” Recomputation = è¡€èµšä¸äºï¼



------

## 4.3 Backward Kernel çš„è®¾è®¡

**ç›®æ ‡**ï¼šç†è§£ä¸ºä»€ä¹ˆéœ€è¦ä¸¤ä¸ª kernelï¼Œä»¥åŠå®ƒä»¬çš„åˆ†å—ç­–ç•¥

### 4.3.1 ä¸ºä»€ä¹ˆéœ€è¦ä¸¤ä¸ª Kernelï¼Ÿ

æ¢¯åº¦å…¬å¼ä¸­ï¼š

- dQ å’Œ dO çš„å½¢çŠ¶ç›¸åŒï¼š`[B, H, S_q, D]`
- dK, dV å’Œ K, V çš„å½¢çŠ¶ç›¸åŒï¼š`[B, H, S_k, D]`

Forward çš„å¹¶è¡Œç­–ç•¥ï¼š

- æ¯ä¸ª program è´Ÿè´£ä¸€ä¸ª Q_blockï¼ˆ`[BLOCK_M, D]`ï¼‰
- æ‰«ææ‰€æœ‰ K/V blocks æ¥è®¡ç®—è¿™ä¸ª Q_block çš„è¾“å‡º

Backward å¦‚æœä¹Ÿæƒ³ç”¨åŒæ ·çš„ç­–ç•¥ï¼š

- è®¡ç®— dQ å¯ä»¥ï¼Œå› ä¸º dQ å’Œ Q å½¢çŠ¶ç›¸åŒ
- ä½†è®¡ç®— dK/dV å°±ä¸è¡Œäº†ï¼š
  - æ¯ä¸ª K/V_block ä¼šè¢«å¤šä¸ª Q_blocks ç”¨åˆ°
  - æ‰€ä»¥éœ€è¦**ç´¯åŠ æ‰€æœ‰ Q_blocks å¯¹è¿™ä¸ª K/V_block æ¢¯åº¦çš„è´¡çŒ®**ï¼Œè¿™å’Œ forward çš„é€»è¾‘ä¸åŒ

**ç»“è®º**ï¼šéœ€è¦ä¸¤ä¸ª kernel

- **dQ kernel**ï¼šå’Œ forward ç±»ä¼¼ï¼Œæ¯ä¸ª program è´Ÿè´£ä¸€ä¸ª Q_block
- **dKV kernel**ï¼šæ¯ä¸ª program è´Ÿè´£ä¸€ä¸ª K/V_blockï¼Œæ‰«ææ‰€æœ‰ Q_blocks å¹¶ç´¯åŠ 

### 4.3.2 dQ Kernel çš„è®¾è®¡

è¾“å…¥ï¼š

- Q, K, Vï¼šå½¢çŠ¶ `[B, H, S_q/S_k, D]`
- dOï¼šå½¢çŠ¶ `[B, H, S_q, D]`
- LSEï¼šå½¢çŠ¶ `[B, H, S_q]`

è¾“å‡ºï¼š

- dQï¼šå½¢çŠ¶ `[B, H, S_q, D]`

ç®—æ³•æµç¨‹ï¼š

```python
for each Q_block:
    dQ_block = 0
    delta_block = row_sum(dO_block âŠ™ O_block)  # é¢„è®¡ç®—, Q_block ç¡®å®š, dO_block å’Œ O_block å°±ç¡®å®š, å› æ­¤ä¸éœ€è¦åœ¨å†…å¾ªç¯è®¡ç®—

    for each K/V_block:
        # é‡å»º P_block
        S_block = Q_block @ K_block.T / sqrt(D)
        P_block = exp(S_block - LSE[:, None])
        
        # è®¡ç®— dP_block
        dP_block = dO_block @ V_block.T
        
        # è®¡ç®— dS_block
        dS_block = P_block âŠ™ (dP_block - delta_block[:, None])
        
        # ç´¯åŠ åˆ° dQ
        dQ_block += dS_block @ K_block / sqrt(D)
    
    write back dQ_block
```

å¾ªç¯å†…éƒ¨ç±»ä¼¼ forwardï¼Œä½†è®¡ç®—çš„æ˜¯æ¢¯åº¦

### 4.3.3 dKV Kernel çš„è®¾è®¡

è¾“å…¥ï¼š

- Q, K, V, dO, LSE

è¾“å‡ºï¼š

- dK, dVï¼šå½¢çŠ¶ `[B, H, S_k, D]`

ç®—æ³•æµç¨‹ï¼š

```python
for each K/V_block:
    dK_block = 0
    dV_block = 0
 
    for each Q_block:
        # é‡å»º P_block (å’Œ dQ kernel ä¸€æ ·)
        S_block = Q_block @ K_block.T / sqrt(D)
        P_block = exp(S_block - LSE[:, None])
        
        # è®¡ç®— dV
        dV_block += P_block.T @ dO_block
        
        # è®¡ç®— delta_block
        delta_block = row_sum(dO_block âŠ™ O_block)
        
        # è®¡ç®— dK (éœ€è¦ dS)
        dP_block = dO_block @ V_block.T
        dS_block = P_block âŠ™ (dP_block - delta_block[:, None])
        dK_block += dS_block.T @ Q_block / sqrt(D)
    
    write back dK_block, dV_block
```

**æ³¨æ„**ï¼š

- éœ€è¦ç´¯åŠ æ‰€æœ‰ Q_blocks å¯¹ K/V æ¢¯åº¦çš„è´¡çŒ®
- dK å’Œ dV å¯ä»¥åœ¨åŒä¸€ä¸ª kernel é‡Œç®—ï¼ˆå…±äº« P çš„é‡å»ºï¼‰



------

## 4.4 dQ Kernel å®ç°

ç»è¿‡å‰é¢å‡ ç« çš„å­¦ä¹ ï¼Œç›¸ä¿¡å¤§å®¶å·²ç»å¯¹ triton æ¯”è¾ƒç†Ÿæ‚‰äº†ï¼Œæ‰€ä»¥ä¹‹åå°±ä¸å†ä½¿ç”¨ python æ¨¡æ‹Ÿï¼Œè€Œæ˜¯ç›´æ¥ç»™å‡º triton çš„ä»£ç å®ç°ã€‚

### 4.4.1 Triton å®ç°

```python
import triton
import triton.language as tl

@triton.jit
def flash_attention_dQ_kernel(
    # -------------------- æŒ‡é’ˆ ------------------------
    Q, K, V, dO, O, LSE,  # è¾“å…¥æŒ‡é’ˆ    
    dQ,                   # è¾“å‡ºæŒ‡é’ˆ

    # -------------------- stride ----------------------
    stride_qb, stride_qh, stride_qs, stride_qd,
    stride_kb, stride_kh, stride_ks, stride_kd,
    stride_vb, stride_vh, stride_vs, stride_vd,
    stride_dob, stride_doh, stride_dos, stride_dod,
    stride_ob, stride_oh, stride_os, stride_od,
    stride_lse_b, stride_lse_h, stride_lse_s,
    stride_dqb, stride_dqh, stride_dqs, stride_dqd,

    # -------------------- ç¼©æ”¾å› å­ --------------------
    scale, # 1 / sqrt(D)

    # -------------------- ç»´åº¦å‚æ•° --------------------
    B, H, S_q, S_k, 
    D: tl.constexpr,

    # -------------------- é…ç½®å‚æ•° --------------------
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,

    # -------------------- Flagå‚æ•° --------------------
    is_causal: tl.constexpr = False,
):
    """
    FlashAttention dQ kernel
    æ¯ä¸ª program è´Ÿè´£è®¡ç®—ä¸€ä¸ª Q_block çš„æ¢¯åº¦
    """
    # è·å–å½“å‰ program çš„ç´¢å¼•
    pid_0 = tl.program_id(0)
    pid_1 = tl.program_id(1)
    batch_idx = pid_1 // H
    head_idx = pid_1 % H
    
    # è®¡ç®—åŸºåœ°å€åç§»
    Q_base = Q + batch_idx * stride_qb + head_idx * stride_qh
    K_base = K + batch_idx * stride_kb + head_idx * stride_kh
    V_base = V + batch_idx * stride_vb + head_idx * stride_vh
    dO_base = dO + batch_idx * stride_dob + head_idx * stride_doh
    O_base = O + batch_idx * stride_ob + head_idx * stride_oh
    LSE_base = LSE + batch_idx * stride_lse_b + head_idx * stride_lse_h
    dQ_base = dQ + batch_idx * stride_dqb + head_idx * stride_dqh
    
    # è®¡ç®— Q_block çš„èµ·å§‹ä½ç½®
    q_block_start = pid_0 * BLOCK_M

    # è¡Œåˆ—ç´¢å¼•
    Sq_offs = q_block_start + tl.arange(0, BLOCK_M)
    d_offs = tl.arange(0, D)

    # maskï¼šå¤„ç†æœ€åä¸€ä¸ª Q_block
    mask_Sq = Sq_offs < S_q
   
    # load Q_block
    q_ptrs = Q_base + Sq_offs[:, None] * stride_qs + d_offs[None, :] * stride_qd
    Q_block = tl.load(q_ptrs, mask=mask_Sq[:, None], other=0.0) # [BLOCK_M, D]
    
    # load dO_block å’Œ O_block
    do_ptrs = dO_base + Sq_offs[:, None] * stride_dos + d_offs[None, :] * stride_dod
    dO_block = tl.load(do_ptrs, mask=mask_Sq[:, None], other=0.0) # [BLOCK_M, D]
    o_ptrs = O_base + Sq_offs[:, None] * stride_os + d_offs[None, :] * stride_od
    O_block = tl.load(o_ptrs, mask=mask_Sq[:, None], other=0.0) # [BLOCK_M, D]
    
    # load LSE_block
    lse_ptrs = LSE_base + Sq_offs * stride_lse_s
    LSE_block = tl.load(lse_ptrs, mask=mask_Sq, other=0.0) # [BLOCK_M,]
    
    # é¢„è®¡ç®— delta = sum(dO âŠ™ O, dim=-1)
    delta_block = tl.sum(dO_block.to(tl.float32) * O_block.to(tl.float32), 
                         axis=-1)  # [BLOCK_M,]
    
    # åˆå§‹åŒ– dQ_block ç´¯åŠ å™¨
    dQ_acc = tl.zeros([BLOCK_M, D], dtype=tl.float32)
    
    LOG2_E = 1.44269504 # log2(e), ç”¨äºtl.exp åˆ° tl.exp2 çš„è½¬åŒ–

    # Causal mask çš„å¾ªç¯è¾¹ç•Œ
    loop_end = q_block_start + BLOCK_M if is_causal else S_k   
    for start_s in range(0, loop_end, BLOCK_N):
        Sk_offs = start_s + tl.arange(0, BLOCK_N)
        mask_Sk = Sk_offs < S_k
        
        # åŠ è½½ K_block å’Œ V_block
        k_ptrs = K_base + Sk_offs[:, None] * stride_ks + d_offs[None, :] * stride_kd
        K_block = tl.load(k_ptrs, mask=mask_Sk[:, None], other=0.0) # [BLOCK_N, D]
        v_ptrs = V_base + Sk_offs[:, None] * stride_vs + d_offs[None, :] * stride_vd
        V_block = tl.load(v_ptrs, mask=mask_Sk[:, None], other=0.0) # [BLOCK_N, D]
        
        # 1. è®¡ç®— S_block = Q_block @ K_block.T * scale
        S_block = tl.dot(Q_block, tl.trans(K_block)) * scale  # [BLOCK_M, BLOCK_N]
        S_block = tl.where(mask_Sk[None, :], S_block, float('-inf')) # [BLOCK_M, BLOCK_N]
        # å¤„ç† padding è¡Œ, ä¹Ÿå¯ä»¥ä¸åŠ è¿™ä¸€å¥, å› ä¸º padding çš„é‚£å‡ è¡Œå¹¶ä¸ä¼šå½±å“åˆ°ç»“æœ
        S_block = tl.where(mask_Sq[:, None], S_block, float('-inf')) 

        # 2. åº”ç”¨ causal mask
        if is_causal:
            q_idx_min = q_block_start
            k_idx_max = start_s + BLOCK_N - 1
            if not (q_idx_min >= k_idx_max):
                causal_mask = Sq_offs[:, None] >= Sk_offs[None, :] # [BLOCK_M, BLOCK_N]
                S_block = tl.where(causal_mask, S_block, float('-inf')) # [BLOCK_M, BLOCK_N]
        
        # 3. é‡å»º P_block = exp(S_block - LSE_block)
        P_block = tl.exp2((S_block - LSE_block[:, None]) * LOG2_E)  # [BLOCK_M, BLOCK_N]
        
        # 4. è®¡ç®— dP_block = dO @ V^T
        dP_block = tl.dot(dO_block, tl.trans(V_block))  # [BLOCK_M, BLOCK_N]
        
        # 5. è®¡ç®— dS_block = P_block âŠ™ (dP_block - delta_block)
        dS_block = P_block * (dP_block - delta_block[:, None])  # [BLOCK_M, BLOCK_N]
        
        # 6. ç´¯åŠ  dQ = dS @ K * scale
        dQ_acc += tl.dot(dS_block.to(tl.float16), K_block) * scale  # [BLOCK_M, D]
    
    # write back to dQ_block
    dq_ptrs = dQ_base + Sq_offs[:, None] * stride_dqs + d_offs[None, :] * stride_dqd
    tl.store(dq_ptrs, dQ_acc, mask=mask_Sq[:, None])
```



### 4.4.2 æ­£ç¡®æ€§éªŒè¯

```python
import torch
import torch.nn.functional as F
from torch.nn.attention import SDPBackend, sdpa_kernel
from torch.amp import autocast

def test_dQ_kernel(B, H, S_q, S_k, D, BLOCK_M, BLOCK_N, is_causal, device):
    torch.manual_seed(42)

    Q = torch.randn((B, H, S_q, D), dtype=torch.float16, device=device)
    K = torch.randn((B, H, S_k, D), dtype=torch.float16, device=device)
    V = torch.randn((B, H, S_k, D), dtype=torch.float16, device=device)
    dO = torch.randn((B, H, S_q, D), dtype=torch.float16, device=device)
    O = torch.empty((B, H, S_q, D), dtype=torch.float16, device=device)
    dQ = torch.empty((B, H, S_q, D), dtype=torch.float16, device=device)
    LSE = torch.empty((B, H, S_q), dtype=torch.float32, device=device)

    # ä½¿ç”¨ PyTorch è®¡ç®—å‚è€ƒç»“æœ
    Q_ref = Q.detach().clone().requires_grad_(True)  # fp16
    K_ref = K.detach().clone().requires_grad_(True)  # fp16
    V_ref = V.detach().clone().requires_grad_(True)  # fp16

    # Forward (ä½¿ç”¨ PyTorch çš„ scaled_dot_product_attention)
    with sdpa_kernel(SDPBackend.FLASH_ATTENTION): # æ˜¾å¼å¯ç”¨ FlashAttention åç«¯
        with autocast(device_type="cuda", dtype=torch.float16):
            O_ref = F.scaled_dot_product_attention(
                Q_ref, 
                K_ref, 
                V_ref,
                attn_mask=None,
                dropout_p=0.0,
                is_causal=is_causal,
            )

    # Backward
    O_ref.backward(dO)
    dQ_ref = Q_ref.grad # å‚è€ƒç»“æœ
    
    # ä½¿ç”¨æˆ‘ä»¬åœ¨ Phase 3 ä¸­å®Œæˆçš„å‰å‘ç®—å­, å…ˆ forward è·å¾— O å’Œ LSE
    from _fwd_LogSumExp import flash_attention_forward_kernel # _fwd_LogSumExpï¼šå­˜æ”¾å‰å‘ç®—å­çš„æ–‡ä»¶å, å¤§å®¶éœ€è¦æ¢æˆè‡ªå·±çš„æ–‡ä»¶è·¯å¾„
    grid = (triton.cdiv(S_q, BLOCK_M), B * H)
    flash_attention_forward_kernel[grid](
        Q, K, V, O,
        LSE, 
        Q.stride(0), Q.stride(1), Q.stride(2), Q.stride(3),
        K.stride(0), K.stride(1), K.stride(2), K.stride(3),
        V.stride(0), V.stride(1), V.stride(2), V.stride(3),
        O.stride(0), O.stride(1), O.stride(2), O.stride(3),
        LSE.stride(0), LSE.stride(1), LSE.stride(2), 
        1 / (D ** 0.5),
        B, H, S_q, S_k, D, 
        BLOCK_M, BLOCK_N,       
        is_causal,
        num_warps=4,
        num_stages=3,
    )
    
    # è®¡ç®— dQ
    flash_attention_dQ_kernel[grid](
        Q, K, V, dO, O, LSE, dQ,        
        Q.stride(0), Q.stride(1), Q.stride(2), Q.stride(3),
        K.stride(0), K.stride(1), K.stride(2), K.stride(3),
        V.stride(0), V.stride(1), V.stride(2), V.stride(3),
        dO.stride(0), dO.stride(1), dO.stride(2), dO.stride(3),
        O.stride(0), O.stride(1), O.stride(2), O.stride(3),
        LSE.stride(0), LSE.stride(1), LSE.stride(2),
        dQ.stride(0), dQ.stride(1), dQ.stride(2), dQ.stride(3),
        1 / (D ** 0.5),
        B, H, S_q, S_k, D,
        BLOCK_M, BLOCK_N, 
        is_causal, 
        num_warps=4,
        num_stages=3,
    )

    return dQ_ref, dQ
```

æ ¡éªŒå‡½æ•°ä¾ç„¶ä½¿ç”¨ Phase 2 çš„ 2.2.3 æ‰€å±•ç¤ºçš„ `verify_results`ï¼Œæµ‹è¯•ç»“æœå¦‚ä¸‹ï¼š

`is_causal = True` æ—¶ï¼š

* å½“ B = 32ï¼ŒH = 8ï¼ŒS_q = 128ï¼ŒS_k = 128ï¼ŒD = 64ï¼ŒBLOCK_M = 64ï¼ŒBLOCK_N = 64ï¼šæµ‹è¯•é€šè¿‡
* å½“ B = 32ï¼ŒH = 8ï¼ŒS_q = 500ï¼ŒS_k = 500ï¼ŒD = 64ï¼ŒBLOCK_M = 64ï¼ŒBLOCK_N = 64ï¼šæµ‹è¯•é€šè¿‡
* â€¦â€¦
* å½“ B = 32ï¼ŒH = 8ï¼ŒS_q = 1024ï¼ŒS_k = 1024ï¼ŒD = 64ï¼ŒBLOCK_M = 64ï¼ŒBLOCK_N = 64ï¼šæµ‹è¯•é€šè¿‡

`is_causal = False` æ—¶ï¼š

* å½“ B = 32ï¼ŒH = 8ï¼ŒS_q = 128ï¼ŒS_k = 128ï¼ŒD = 64ï¼ŒBLOCK_M = 64ï¼ŒBLOCK_N = 64ï¼šæµ‹è¯•é€šè¿‡
* å½“ B = 32ï¼ŒH = 8ï¼ŒS_q = 500ï¼ŒS_k = 500ï¼ŒD = 64ï¼ŒBLOCK_M = 64ï¼ŒBLOCK_N = 64ï¼šæµ‹è¯•é€šè¿‡
* â€¦â€¦
* å½“ B = 32ï¼ŒH = 8ï¼ŒS_q = 1024ï¼ŒS_k = 1024ï¼ŒD = 64ï¼ŒBLOCK_M = 64ï¼ŒBLOCK_N = 64ï¼šæµ‹è¯•é€šè¿‡

------

## 4.5 dKV Kernel å®ç°

### 4.5.1 Triton å®ç°

```python
import triton
import triton.language as tl

@triton.jit
def flash_attention_dKV_kernel(
    # -------------------- æŒ‡é’ˆ ------------------------
    Q, K, V, dO, O, LSE,  # è¾“å…¥æŒ‡é’ˆ    
    dK, dV,               # è¾“å‡ºæŒ‡é’ˆ

    # -------------------- stride ----------------------
    stride_qb, stride_qh, stride_qs, stride_qd,
    stride_kb, stride_kh, stride_ks, stride_kd,
    stride_vb, stride_vh, stride_vs, stride_vd,
    stride_dob, stride_doh, stride_dos, stride_dod,
    stride_ob, stride_oh, stride_os, stride_od,
    stride_lse_b, stride_lse_h, stride_lse_s,
    stride_dkb, stride_dkh, stride_dks, stride_dkd,
    stride_dvb, stride_dvh, stride_dvs, stride_dvd,

    # -------------------- ç¼©æ”¾å› å­ --------------------
    scale, # 1 / sqrt(D)

    # -------------------- ç»´åº¦å‚æ•° --------------------
    B, H, S_q, S_k, 
    D: tl.constexpr,

    # -------------------- é…ç½®å‚æ•° --------------------
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,

    # -------------------- Flagå‚æ•° --------------------
    is_causal: tl.constexpr = False,
):
    """
    FlashAttention dKV kernel
    æ¯ä¸ª program è´Ÿè´£è®¡ç®—ä¸€ä¸ª K/V_block çš„æ¢¯åº¦
    """
    # è·å–å½“å‰ program çš„ç´¢å¼•
    pid_0 = tl.program_id(0)
    pid_1 = tl.program_id(1)
    batch_idx = pid_1 // H
    head_idx = pid_1 % H
    
    # è®¡ç®—åŸºåœ°å€åç§»
    Q_base = Q + batch_idx * stride_qb + head_idx * stride_qh
    K_base = K + batch_idx * stride_kb + head_idx * stride_kh
    V_base = V + batch_idx * stride_vb + head_idx * stride_vh
    dO_base = dO + batch_idx * stride_dob + head_idx * stride_doh
    O_base = O + batch_idx * stride_ob + head_idx * stride_oh
    LSE_base = LSE + batch_idx * stride_lse_b + head_idx * stride_lse_h
    dK_base = dK + batch_idx * stride_dkb + head_idx * stride_dkh
    dV_base = dV + batch_idx * stride_dvb + head_idx * stride_dvh
    
    # è®¡ç®— K/V_block çš„èµ·å§‹ä½ç½®
    kv_block_start = pid_0 * BLOCK_N

    # è¡Œåˆ—ç´¢å¼•
    Sk_offs = kv_block_start + tl.arange(0, BLOCK_N)
    d_offs = tl.arange(0, D)

    # maskï¼šå¤„ç†æœ€åä¸€ä¸ª K/V_block
    mask_Sk = Sk_offs < S_k

    # load K/V_block
    k_ptrs = K_base + Sk_offs[:, None] * stride_ks + d_offs[None, :] * stride_kd
    K_block = tl.load(k_ptrs, mask=mask_Sk[:, None], other=0.0) # [BLOCK_N, D]
    v_ptrs = V_base + Sk_offs[:, None] * stride_vs + d_offs[None, :] * stride_vd
    V_block = tl.load(v_ptrs, mask=mask_Sk[:, None], other=0.0) # [BLOCK_N, D]
    
    # åˆå§‹åŒ– dK_block, dV_block ç´¯åŠ å™¨
    dK_acc = tl.zeros([BLOCK_N, D], dtype=tl.float32)
    dV_acc = tl.zeros([BLOCK_N, D], dtype=tl.float32)
    
    LOG2_E = 1.44269504 # log2(e), ç”¨äºtl.exp åˆ° tl.exp2 çš„è½¬åŒ–
    
    # Causal mask çš„å¾ªç¯è¾¹ç•Œ
    loop_start = kv_block_start if is_causal else 0
    for start_s in range(loop_start, S_q, BLOCK_M):
        Sq_offs = start_s + tl.arange(0, BLOCK_M)
        mask_Sq = Sq_offs < S_q
        
        # load Q_block, dO_block, O_block, LSE_block
        q_ptrs = Q_base + Sq_offs[:, None] * stride_qs + d_offs[None, :] * stride_qd
        Q_block = tl.load(q_ptrs, mask=mask_Sq[:, None], other=0.0) # [BLOCK_M, D]
        do_ptrs = dO_base + Sq_offs[:, None] * stride_dos + d_offs[None, :] * stride_dod
        dO_block = tl.load(do_ptrs, mask=mask_Sq[:, None], other=0.0) # [BLOCK_M, D]
        o_ptrs = O_base + Sq_offs[:, None] * stride_os + d_offs[None, :] * stride_od
        O_block = tl.load(o_ptrs, mask=mask_Sq[:, None], other=0.0) # [BLOCK_M, D]
        lse_ptrs = LSE_base + Sq_offs * stride_lse_s
        LSE_block = tl.load(lse_ptrs, mask=mask_Sq, other=0.0) # [BLOCK_M,]
        
        # 1. è®¡ç®— S_block = Q_block @ K_block.T * scale
        S_block = tl.dot(Q_block, tl.trans(K_block)) * scale  # [BLOCK_M, BLOCK_N]
        S_block = tl.where(mask_Sk[None, :], S_block, float('-inf')) # [BLOCK_M, BLOCK_N]
        # å¤„ç† padding è¡Œ, ä¸åŠ è¿™ä¸€è¡Œä¼šå¯¼è‡´ P_block çš„ padding è¡Œ=1, è€Œä¸æ˜¯0, ä»è€Œå½±å“ dV_acc
        S_block = tl.where(mask_Sq[:, None], S_block, float('-inf')) 
        
        # 2. åº”ç”¨ causal mask
        if is_causal:
            q_idx_min = start_s
            k_idx_max = kv_block_start + BLOCK_N - 1
            if not (q_idx_min >= k_idx_max):
                causal_mask = Sq_offs[:, None] >= Sk_offs[None, :] # [BLOCK_M, BLOCK_N]
                S_block = tl.where(causal_mask, S_block, float('-inf')) # [BLOCK_M, BLOCK_N]
        
        # 3. é‡å»º P_block = exp(S_block - LSE_block)
        P_block = tl.exp2((S_block - LSE_block[:, None]) * LOG2_E)  # [BLOCK_M, BLOCK_N]

        # 4. ç´¯åŠ  dV = P^T @ dO
        dV_acc += tl.dot(tl.trans(P_block).to(tl.float16), dO_block)   # [BLOCK_N, D]
        
        # 5. è®¡ç®— dP_block = dO @ V^T
        dP_block = tl.dot(dO_block, tl.trans(V_block))  # [BLOCK_M, BLOCK_N]
        
        # 6. è®¡ç®— delta = sum(dO âŠ™ O, dim=-1)
        delta_block = tl.sum(dO_block.to(tl.float32) * O_block.to(tl.float32), 
                            axis=-1)  # [BLOCK_M,]
    
        # 7. è®¡ç®— dS_block = P_block âŠ™ (dP_block - delta_block)
        dS_block = P_block * (dP_block - delta_block[:, None])  # [BLOCK_M, BLOCK_N]
        
        # 8. ç´¯åŠ  dK = dS^T @ Q * scale
        dK_acc += tl.dot(tl.trans(dS_block).to(tl.float16), Q_block) * scale  # [BLOCK_N, D]
    
    # write back to dK/dV_block
    dk_ptrs = dK_base + Sk_offs[:, None] * stride_dks + d_offs[None, :] * stride_dkd
    tl.store(dk_ptrs, dK_acc, mask=mask_Sk[:, None])
    dv_ptrs = dV_base + Sk_offs[:, None] * stride_dvs + d_offs[None, :] * stride_dvd
    tl.store(dv_ptrs, dV_acc, mask=mask_Sk[:, None])
```



### 4.5.2 æ­£ç¡®æ€§éªŒè¯

```python
import torch
import torch.nn.functional as F
from torch.nn.attention import SDPBackend, sdpa_kernel
from torch.amp import autocast

def test_dKV_kernel(B, H, S_q, S_k, D, BLOCK_M, BLOCK_N, is_causal, device):
    torch.manual_seed(42)

    Q = torch.randn((B, H, S_q, D), dtype=torch.float16, device=device)
    K = torch.randn((B, H, S_k, D), dtype=torch.float16, device=device)
    V = torch.randn((B, H, S_k, D), dtype=torch.float16, device=device)
    dO = torch.randn((B, H, S_q, D), dtype=torch.float16, device=device)
    O = torch.empty((B, H, S_q, D), dtype=torch.float16, device=device)
    dK = torch.empty((B, H, S_k, D), dtype=torch.float16, device=device)
    dV = torch.empty((B, H, S_k, D), dtype=torch.float16, device=device)
    LSE = torch.empty((B, H, S_q), dtype=torch.float32, device=device)

    # ä½¿ç”¨ PyTorch è®¡ç®—å‚è€ƒç»“æœ
    Q_ref = Q.detach().clone().requires_grad_(True)  # fp16
    K_ref = K.detach().clone().requires_grad_(True)  # fp16
    V_ref = V.detach().clone().requires_grad_(True)  # fp16

    # Forward (ä½¿ç”¨ PyTorch çš„ scaled_dot_product_attention)
    with sdpa_kernel(SDPBackend.FLASH_ATTENTION): # æ˜¾å¼å¯ç”¨ FlashAttention åç«¯
        with autocast(device_type="cuda", dtype=torch.float16):
            O_ref = F.scaled_dot_product_attention(
                Q_ref, 
                K_ref, 
                V_ref,
                attn_mask=None,
                dropout_p=0.0,
                is_causal=is_causal,
            )

    # Backward
    O_ref.backward(dO)
    dK_ref = K_ref.grad
    dV_ref = V_ref.grad
    
    # ä½¿ç”¨æˆ‘ä»¬çš„ kernel, å…ˆ forward è·å¾— O å’Œ LSE
    from _fwd_LogSumExp import flash_attention_forward_kernel # _fwd_LogSumExpï¼šå­˜æ”¾å‰å‘ç®—å­çš„æ–‡ä»¶å, å¤§å®¶éœ€è¦æ¢æˆè‡ªå·±çš„æ–‡ä»¶è·¯å¾„
    grid_fwd = (triton.cdiv(S_q, BLOCK_M), B * H)
    flash_attention_forward_kernel[grid_fwd](
        Q, K, V, O,
        LSE, 
        Q.stride(0), Q.stride(1), Q.stride(2), Q.stride(3),
        K.stride(0), K.stride(1), K.stride(2), K.stride(3),
        V.stride(0), V.stride(1), V.stride(2), V.stride(3),
        O.stride(0), O.stride(1), O.stride(2), O.stride(3),
        LSE.stride(0), LSE.stride(1), LSE.stride(2), 
        1 / (D ** 0.5),
        B, H, S_q, S_k, D, 
        BLOCK_M, BLOCK_N,       
        is_causal,
        num_warps=4,
        num_stages=3,
    )
    
    # è®¡ç®— dK/V
    grid_bwd = (triton.cdiv(S_k, BLOCK_N), B * H)
    flash_attention_dKV_kernel[grid_bwd](
        Q, K, V, dO, O, LSE, dK, dV,      
        Q.stride(0), Q.stride(1), Q.stride(2), Q.stride(3),
        K.stride(0), K.stride(1), K.stride(2), K.stride(3),
        V.stride(0), V.stride(1), V.stride(2), V.stride(3),
        dO.stride(0), dO.stride(1), dO.stride(2), dO.stride(3),
        O.stride(0), O.stride(1), O.stride(2), O.stride(3),
        LSE.stride(0), LSE.stride(1), LSE.stride(2),
        dK.stride(0), dK.stride(1), dK.stride(2), dK.stride(3),
        dV.stride(0), dV.stride(1), dV.stride(2), dV.stride(3),
        1 / (D ** 0.5),
        B, H, S_q, S_k, D,
        BLOCK_M, BLOCK_N, 
        is_causal, 
        num_warps=4,
        num_stages=3,
    )

    return dK_ref, dV_ref, dK, dV
```

æ ¡éªŒå‡½æ•°åŒä¸Šï¼Œæµ‹è¯•ç»“æœå¦‚ä¸‹ï¼š

`is_causal = True` æ—¶ï¼š

* å½“ B = 32ï¼ŒH = 8ï¼ŒS_q = 128ï¼ŒS_k = 128ï¼ŒD = 64ï¼ŒBLOCK_M = 64ï¼ŒBLOCK_N = 64ï¼šæµ‹è¯•é€šè¿‡
* å½“ B = 32ï¼ŒH = 8ï¼ŒS_q = 500ï¼ŒS_k = 500ï¼ŒD = 64ï¼ŒBLOCK_M = 64ï¼ŒBLOCK_N = 64ï¼šæµ‹è¯•é€šè¿‡
* â€¦â€¦
* å½“ B = 32ï¼ŒH = 8ï¼ŒS_q = 1024ï¼ŒS_k = 1024ï¼ŒD = 64ï¼ŒBLOCK_M = 64ï¼ŒBLOCK_N = 64ï¼šæµ‹è¯•é€šè¿‡

`is_causal = False` æ—¶ï¼š

* å½“ B = 32ï¼ŒH = 8ï¼ŒS_q = 128ï¼ŒS_k = 128ï¼ŒD = 64ï¼ŒBLOCK_M = 64ï¼ŒBLOCK_N = 64ï¼šæµ‹è¯•é€šè¿‡
* å½“ B = 32ï¼ŒH = 8ï¼ŒS_q = 500ï¼ŒS_k = 500ï¼ŒD = 64ï¼ŒBLOCK_M = 64ï¼ŒBLOCK_N = 64ï¼šæµ‹è¯•é€šè¿‡
* â€¦â€¦
* å½“ B = 32ï¼ŒH = 8ï¼ŒS_q = 1024ï¼ŒS_k = 1024ï¼ŒD = 64ï¼ŒBLOCK_M = 64ï¼ŒBLOCK_N = 64ï¼šæµ‹è¯•é€šè¿‡



------

## 4.6 PyTorch Autograd é›†æˆ

**ç›®æ ‡**ï¼šæŠŠæˆ‘ä»¬å†™å¥½çš„ forward / backward kernels åŒ…è£…æˆä¸€ä¸ª PyTorch ç®—å­ï¼Œä½¿å…¶å¯ä»¥å‚ä¸çœŸæ­£çš„æ¨¡å‹è®­ç»ƒ

### 4.6.1 ç†è§£ autograd.Function çš„å·¥ä½œæµç¨‹

**Forward kernel è¿”å›ä»€ä¹ˆï¼Ÿ**

- `O`ï¼šæ³¨æ„åŠ›è¾“å‡ºï¼Œå½¢çŠ¶ `[B, H, S_q, D]`
- `LSE`ï¼šæ¯ä¸€è¡Œ softmax çš„ç»Ÿè®¡é‡ï¼ˆLogSumExpï¼‰ï¼Œå½¢çŠ¶ `[B, H, S_q]`

**Backward kernel éœ€è¦ä»€ä¹ˆï¼Ÿ**

- `Q, K, V`ï¼šç”¨äºé‡ç®— `S` ä¸ `P`
- `O, dO`ï¼šç”¨äºè®¡ç®— `delta = row_sum(dO âŠ™ O)`
- `LSE`ï¼šç”¨äºé‡å»º `P = exp(S - LSE[:, None])`

**ctx é‡Œå­˜ä»€ä¹ˆï¼Ÿ**

```
ctx.save_for_backward(Q, K, V, O, LSE)
ctx.is_causal = is_causal
```

`dO` ç”±å¤–éƒ¨ä¼ å…¥ï¼Œä¸ä½¿ç”¨ `ctx` å­˜å‚¨ã€‚

### 4.6.2 å‡†å¤‡ Python å°è£…

ä¸ºäº†è®© `torch.autograd.Function` æ›´ç®€æ´ï¼Œéœ€è¦å‡†å¤‡ä¸¤ä¸ª Python åŒ…è£…å‡½æ•°ï¼š

- `flash_attention_forward(Q, K, V, is_causal)`ï¼šè´Ÿè´£åˆ†é… `O/LSE` å¹¶ launch forward kernel
- `flash_attention_backward(Q, K, V, O, dO, LSE, is_causal)`ï¼šè´Ÿè´£åˆ†é… `dQ/dK/dV` å¹¶ launch backward kernels

```python
def flash_attention_forward(Q, K, V, is_causal):
    """
    è´Ÿè´£åˆ†é… O/LSE å¹¶ launch forward kernel
    """
    B, H, S_q, D = Q.shape
    _, _, S_k, _ = K.shape
    BLOCK_M = 64
    BLOCK_N = 64

    device = Q.device
    dtype = Q.dtype
    O = torch.empty((B, H, S_q, D), dtype=dtype, device=device)
    LSE = torch.empty((B, H, S_q), dtype=torch.float32, device=device)

    grid = (triton.cdiv(S_q, BLOCK_M), B * H)
    flash_attention_forward_kernel[grid](
        Q, K, V, O, LSE,                     
        Q.stride(0), Q.stride(1), Q.stride(2), Q.stride(3),  
        K.stride(0), K.stride(1), K.stride(2), K.stride(3),  
        V.stride(0), V.stride(1), V.stride(2), V.stride(3),  
        O.stride(0), O.stride(1), O.stride(2), O.stride(3), 
        LSE.stride(0), LSE.stride(1), LSE.stride(2),         
        1 / (D ** 0.5), 
        B, H, S_q, S_k, D,
        BLOCK_M, BLOCK_N,
        is_causal,
        num_warps=4,
        num_stages=3,
    )
    return O, LSE

def flash_attention_backward(Q, K, V, O, dO, LSE, is_causal):
    """
    è´Ÿè´£åˆ†é… dQ/dK/dV å¹¶ launch backward kernels
    """
    B, H, S_q, D = Q.shape
    _, _, S_k, _ = K.shape
    BLOCK_M = 64
    BLOCK_N = 64

    device = Q.device
    dtype = Q.dtype
    dQ = torch.empty((B, H, S_q, D), dtype=dtype, device=device)
    dK = torch.empty((B, H, S_k, D), dtype=dtype, device=device)
    dV = torch.empty((B, H, S_k, D), dtype=dtype, device=device)

    # è®¡ç®— dQ
    grid_Q = (triton.cdiv(S_q, BLOCK_M), B * H)
    flash_attention_dQ_kernel[grid_Q](
        Q, K, V, dO, O, LSE, dQ,        
        Q.stride(0), Q.stride(1), Q.stride(2), Q.stride(3),
        K.stride(0), K.stride(1), K.stride(2), K.stride(3),
        V.stride(0), V.stride(1), V.stride(2), V.stride(3),
        dO.stride(0), dO.stride(1), dO.stride(2), dO.stride(3),
        O.stride(0), O.stride(1), O.stride(2), O.stride(3),
        LSE.stride(0), LSE.stride(1), LSE.stride(2),
        dQ.stride(0), dQ.stride(1), dQ.stride(2), dQ.stride(3),
        1 / (D ** 0.5),
        B, H, S_q, S_k, D,
        BLOCK_M, BLOCK_N, 
        is_causal, 
        num_warps=4,
        num_stages=3,
    )

    # è®¡ç®— dK/V
    grid_KV = (triton.cdiv(S_k, BLOCK_N), B * H)
    flash_attention_dKV_kernel[grid_KV](
        Q, K, V, dO, O, LSE, dK, dV,      
        Q.stride(0), Q.stride(1), Q.stride(2), Q.stride(3),
        K.stride(0), K.stride(1), K.stride(2), K.stride(3),
        V.stride(0), V.stride(1), V.stride(2), V.stride(3),
        dO.stride(0), dO.stride(1), dO.stride(2), dO.stride(3),
        O.stride(0), O.stride(1), O.stride(2), O.stride(3),
        LSE.stride(0), LSE.stride(1), LSE.stride(2),
        dK.stride(0), dK.stride(1), dK.stride(2), dK.stride(3),
        dV.stride(0), dV.stride(1), dV.stride(2), dV.stride(3),
        1 / (D ** 0.5),
        B, H, S_q, S_k, D,
        BLOCK_M, BLOCK_N, 
        is_causal, 
        num_warps=4,
        num_stages=3,
    )

    return dQ, dK, dV
```

### 4.6.3 å®šä¹‰ torch.autograd.Function

```python
class FlashAttentionFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, Q, K, V, is_causal: bool):
        assert Q.is_cuda and K.is_cuda and V.is_cuda
        assert Q.dtype in (torch.float16, torch.bfloat16)
        assert Q.shape[-1] == K.shape[-1] == V.shape[-1]
        assert Q.ndim == 4 and K.ndim == 4 and V.ndim == 4
        # 1. ç¡®ä¿è¾“å…¥è¿ç»­
        Q_ = Q.contiguous()
        K_ = K.contiguous()
        V_ = V.contiguous()
        # 2. è°ƒç”¨ Forward Kernel
        # æ³¨æ„ï¼šè¿™é‡Œè°ƒç”¨çš„æ˜¯ä½ å°è£…å¥½çš„ Python å¯åŠ¨å‡½æ•°
        O, LSE = flash_attention_forward(Q_, K_, V_, is_causal)
        # 3. ä¿å­˜ç”¨äº Backward çš„ Tensor
        ctx.save_for_backward(Q_, K_, V_, O, LSE)
        # 4. ä¿å­˜é Tensor å‚æ•°
        ctx.is_causal = is_causal

        return O

    @staticmethod
    def backward(ctx, dO):
        # 1. å–å‡ºä¿å­˜çš„ Tensor
        Q, K, V, O, LSE = ctx.saved_tensors
        # 2. ç¡®ä¿è¾“å…¥è¿ç»­
        dO_ = dO.contiguous()
        # 3. è°ƒç”¨ Backward Kernel
        # æ³¨æ„ï¼šè¿™é‡Œè°ƒç”¨çš„ä¹Ÿæ˜¯å°è£…å¥½çš„ Python å¯åŠ¨å‡½æ•°
        dQ, dK, dV = flash_attention_backward(
            Q, K, V, O, dO_, LSE, ctx.is_causal
        )        
        # 4. è¿”å›æ¢¯åº¦
        # è¿”å›å€¼çš„é¡ºåºå¿…é¡»ä¸ forward çš„å‚æ•°é¡ºåºå®Œå…¨ä¸€è‡´
        # Q, K, V å¯¹åº”çš„æ¢¯åº¦åˆ†åˆ«æ˜¯ dQ, dK, dV
        # is_causal ä¸æ˜¯ Tensorï¼Œä¸éœ€è¦æ¢¯åº¦ï¼Œè¿”å› None
        return dQ, dK, dV, None
```

ä¸ºäº†æ–¹ä¾¿ä½¿ç”¨ï¼Œæˆ‘ä»¬åœ¨å¤–éƒ¨å†åŒ…ä¸€å±‚ç®€å•çš„ APIï¼š

```python
def flash_attention(Q, K, V, is_causal=False):
    return FlashAttentionFunction.apply(Q, K, V, is_causal)
```

### 4.6.4 æµ‹è¯•æ¢¯åº¦æ­£ç¡®æ€§

```python
def compare_with_sdpa(Q, K, V, is_causal):
    Q_ref = Q.detach().clone().requires_grad_(True)
    K_ref = K.detach().clone().requires_grad_(True)
    V_ref = V.detach().clone().requires_grad_(True)

    # reference
    with sdpa_kernel(SDPBackend.FLASH_ATTENTION): # æ˜¾å¼å¯ç”¨ FlashAttention åç«¯
        with autocast(device_type="cuda", dtype=torch.float16):
            O_ref = F.scaled_dot_product_attention(
                Q_ref, 
                K_ref, 
                V_ref,
                attn_mask=None,
                dropout_p=0.0,
                is_causal=is_causal,
            )

    dO = torch.randn_like(O_ref)

    O_ref.backward(dO)
    dQ_ref, dK_ref, dV_ref = Q_ref.grad, K_ref.grad, V_ref.grad

    # My FlashAttention
    Q_ = Q.detach().clone().requires_grad_(True)
    K_ = K.detach().clone().requires_grad_(True)
    V_ = V.detach().clone().requires_grad_(True)

    O = flash_attention(Q_, K_, V_, is_causal=is_causal)
    O.backward(dO)
    dQ, dK, dV = Q_.grad, K_.grad, V_.grad

    # verify results
    from _verify_func import verify_results # Phase 2 çš„ 2.2.3 ä¸­çš„ verify_results å‡½æ•°
    print(f"{"=" * 30} dQ test {"=" * 30}")
    verify_results(dQ_ref, dQ)
    print(f"{"=" * 30} dK test {"=" * 30}")
    verify_results(dK_ref, dK)
    print(f"{"=" * 30} dV test {"=" * 30}")
    verify_results(dV_ref, dV)

DEVICE = torch.device(torch.cuda.current_device())
B = 32
H = 8
S_q = 1024
S_k = 1024
D = 64

Q = torch.randn((B, H, S_q, D), dtype=torch.float16, device=DEVICE)
K = torch.randn((B, H, S_k, D), dtype=torch.float16, device=DEVICE)
V = torch.randn((B, H, S_k, D), dtype=torch.float16, device=DEVICE)

compare_with_sdpa(Q, K, V, is_causal=True) # âœ… Test Passed!
```

> ç”±äº FlashAttention å·¥ä½œåœ¨ fp16/bf16 ç²¾åº¦å¹¶åŒ…å«æŒ‡æ•°è¿ç®—ï¼Œæ•°å€¼å¾®åˆ†çš„ `gradcheck` åœ¨ä¸­ç­‰è§„æ¨¡ä¸‹å®¹æ˜“è¯¯æŠ¥ã€‚å› æ­¤æˆ‘ä»¬é‡‡ç”¨ä¸å·¥ä¸šç•Œä¸€è‡´çš„æ–¹å¼ï¼šä½¿ç”¨ PyTorch åŸç”Ÿå®ç°ä½œä¸ºæ•°å€¼å‚è€ƒè¿›è¡Œæ­£ç¡®æ€§æ ¡éªŒã€‚

------

## 4.7 å°ç»“

### 4.7.1 å›é¡¾ recomputation çš„æ™ºæ…§

åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬æ”»å…‹äº† FlashAttention æœ€å…·æŒ‘æˆ˜æ€§çš„éƒ¨åˆ†â€”â€”åå‘ä¼ æ’­ã€‚æˆ‘ä»¬ä¹Ÿæ·±åˆ»ä½“ä¼šåˆ°äº†ç®—æ³•è®¾è®¡ä¸­çš„æƒè¡¡ä¹‹ç¾ï¼š

- **ç©ºé—´æ¢æ—¶é—´ï¼Ÿä¸ï¼Œæ˜¯è®¡ç®—æ¢ç©ºé—´ï¼ä¹Ÿæ¢æ—¶é—´ï¼**ï¼šé€šå¸¸æˆ‘ä»¬è®¤ä¸ºé‡è®¡ç®—ï¼ˆRecomputationï¼‰ä¼šæ‹–æ…¢é€Ÿåº¦ï¼Œä½†åœ¨ Memory Wallï¼ˆå†…å­˜å¢™ï¼‰æ—¥ç›Šä¸¥å³»çš„ä»Šå¤©ï¼Œåˆ©ç”¨é«˜é€Ÿçš„ Tensor Core é‡æ–°è®¡ç®— $S$ å’Œ $P$ çŸ©é˜µï¼Œç«Ÿç„¶æ¯”ä» HBM ä¸­è¯»å–å®ƒä»¬è¿˜è¦å¿«ã€‚
- **ç»Ÿè®¡é‡ (LSE) çš„å¦™ç”¨**ï¼šæˆ‘ä»¬ä¸éœ€è¦ä¿å­˜å·¨å¤§çš„ $O(N^2)$ çš„ $P$ çŸ©é˜µï¼Œåªéœ€è¦ä¿å­˜ $O(N)$ å¤§å°çš„ $LSE$ ç»Ÿè®¡é‡ï¼Œå°±èƒ½å®Œç¾è¿˜åŸå‡º Softmax çš„ç»“æœã€‚è¿™æ˜¯ FlashAttention èƒ½å¤ŸèŠ‚çœæ˜¾å­˜ï¼ŒåŠ é€Ÿè®¡ç®—çš„å…³é”®ã€‚

### 4.7.2 æˆ‘ä»¬ç°åœ¨ç«™åœ¨å“ªé‡Œï¼Ÿ

åˆ°è¿™é‡Œä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»**ä»é›¶å®ç°**äº†ä¸€ä¸ªå…·å¤‡ä»¥ä¸‹èƒ½åŠ›çš„ **FlashAttention ç®—å­**ï¼ğŸ‘ğŸ‘ğŸ‘

* **æ­£ç¡®æ€§**ï¼šé€šè¿‡äº† PyTorch åŸç”Ÿç®—å­çš„æ•°å€¼å¯¹é½æµ‹è¯•ã€‚

* **åŠŸèƒ½æ€§**ï¼šæ”¯æŒ Batchã€Multi-Headã€Causal Mask å’Œåå‘ä¼ æ’­ï¼Œå¹¶æ¥å…¥äº† `autograd` ç³»ç»Ÿã€‚

### 4.7.3 Phase 5 é¢„å‘Šï¼šæè‡´æ€§èƒ½ä¼˜åŒ–

è™½ç„¶æˆ‘ä»¬çš„ç®—å­å·²ç»â€œèƒ½ç”¨â€äº†ï¼Œä½†åœ¨â€œå¥½ç”¨â€ä¹‹å‰ï¼Œè¿˜æœ‰ä¸€æ®µè·¯è¦èµ°ã€‚ç›®å‰çš„å®ç°ä¸­ï¼Œæˆ‘ä»¬è¿˜å­˜åœ¨è®¸å¤šæ€§èƒ½ç“¶é¢ˆï¼š

- kernel å†…å¤§é‡åœ°å€è®¡ç®—å¼•å…¥é¢å¤–çš„æŒ‡ä»¤ä¸å¯„å­˜å™¨å ç”¨ï¼Œæ€ä¹ˆè§£å†³ï¼Ÿ
- æµæ°´çº¿ï¼ˆPipelineï¼‰æ˜¯å¦é‡å ï¼Ÿæœ‰æ²¡æœ‰æ°”æ³¡ï¼Ÿ
- Block å¤§å°é€‰æ‹©æ˜¯å¦æœ€ä¼˜ï¼Ÿ
- â€¦â€¦

åœ¨æ¥ä¸‹æ¥çš„ **Phase 5** ä¸­ï¼Œæˆ‘ä»¬å°†ä¸å†å…³æ³¨æ–°çš„åŠŸèƒ½ï¼Œè€Œæ˜¯å¸¦ä¸Šâ€œæ€§èƒ½æ˜¾å¾®é•œâ€ï¼Œæ·±å…¥ GPU æ¶æ„çš„å¾®è§‚ä¸–ç•Œã€‚æˆ‘ä»¬å°†å­¦ä¹  Autotuneã€TensorDescriptorã€è®¡ç®—æµç¨‹ä¼˜åŒ–ç­‰é«˜çº§æŠ€å·§ï¼Œè®©æˆ‘ä»¬çš„ FlashAttention çœŸæ­£é£èµ·æ¥ï¼

å‡†å¤‡å¥½å‹æ¦¨ GPU çš„æœ€åä¸€æ»´ç®—åŠ›äº†å—ï¼Ÿè®©æˆ‘ä»¬è¿›å…¥ Phase 5 å§ï¼

---

## 4.8 é™„å½•

**ç›®æ ‡**ï¼šè¯¦ç»†æ¨å¯¼æ ‡å‡† Attention çš„ backward æ¢¯åº¦å…¬å¼

> è¿™éƒ¨åˆ†å†…å®¹è¾ƒä¸ºæ•°å­¦åŒ–ï¼Œå¦‚æœä½ åªå…³å¿ƒç»“è®ºï¼Œå¯ä»¥è·³è¿‡ã€‚ä½†å¦‚æœä½ æƒ³æ·±å…¥ç†è§£ï¼Œå»ºè®®ä»”ç»†é˜…è¯»ã€‚

### A. ç¬¦å·å®šä¹‰

- $Q \in \mathbb{R}^{S_q \times D}$ï¼šQuery çŸ©é˜µ
- $K,V \in \mathbb{R}^{S_k \times D}$ï¼šKey / Value çŸ©é˜µ  
- $S \in \mathbb{R}^{S_q \times S_k}$ï¼šScore çŸ©é˜µ  
- $P \in \mathbb{R}^{S_q \times S_k}$ï¼šæ³¨æ„åŠ›æƒé‡çŸ©é˜µ
- $O \in \mathbb{R}^{S_q \times D}$ï¼šAttention è¾“å‡º
- $dO \in \mathbb{R}^{S_q \times D}$ï¼šAttention è¾“å‡ºçš„æ¢¯åº¦ï¼ˆå·²çŸ¥ï¼‰

æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ±‚ï¼š

- $dQ \in \mathbb{R}^{S_q \times D}$
- $dK,dV \in \mathbb{R}^{S_k \times D}$

### B. dV å’Œ dP çš„æ¨å¯¼

```python
O = P @ V
```

æ ¹æ®**çŸ©é˜µä¹˜æ³•çš„æ¢¯åº¦å…¬å¼**ï¼ˆè§ 4.1.1 èŠ‚ï¼‰ï¼š

```python
dV = P^T @ dO
dP = dO @ V^T
```

**éªŒè¯ç»´åº¦**ï¼š

- `P` çš„å½¢çŠ¶ï¼š`[S_q, S_k]`ï¼Œ`V` çš„å½¢çŠ¶ï¼š`[S_k, D]`ï¼Œ`dO` çš„å½¢çŠ¶ï¼š`[S_q, D]`
- `dV = P^T @ dO` çš„å½¢çŠ¶ï¼š`[S_k, S_q] @ [S_q, D] = [S_k, D]` âœ…
- `dP = dO @ V^T` çš„å½¢çŠ¶ï¼š`[S_q, D] @ [D, S_k] = [S_q, S_k]` âœ…

### C. dS çš„æ¨å¯¼ï¼ˆæ ¸å¿ƒéƒ¨åˆ†ï¼‰

#### C.1 Row-wise Softmax çš„å®šä¹‰

å¯¹äºçŸ©é˜µ S çš„ç¬¬ i è¡Œï¼Œsoftmax çš„å®šä¹‰æ˜¯ï¼š

```python
P_ij = exp(S_ij) / sum_k(exp(S_ik))
```

ä¸ºäº†æ•°å€¼ç¨³å®šæ€§ï¼Œå®é™…è®¡ç®—æ—¶ä¼šå‡å»æœ€å¤§å€¼ï¼š

```python
m_i = max_k(S_ik)
P_ij = exp(S_ij - m_i) / sum_k(exp(S_ik - m_i))
```

æ³¨æ„ï¼šè¯¥å˜æ¢å¹¶ä¸ä¼šæ”¹å˜å‡½æ•°å€¼ï¼Œç”±äºæ¢¯åº¦åªä¾èµ–äºå‡½æ•°æœ¬èº«ï¼Œè€Œå‡½æ•°æœ¬èº«å¹¶æœªæ”¹å˜ï¼Œæ‰€ä»¥åœ¨æ¨å¯¼æ¢¯åº¦æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥å¿½ç•¥è¿™ä¸ªæŠ€å·§ï¼Œç›´æ¥ç”¨åŸå§‹å®šä¹‰ã€‚

#### C.2 å…ˆç®— Softmax çš„ Jacobian çŸ©é˜µ

å¯¹äºç¬¬ i è¡Œï¼Œsoftmax æ˜¯ä¸€ä¸ªä» $\mathbb{R^{S_k}}$ åˆ° $\mathbb{R^{S_k}}$ çš„å‡½æ•°ï¼š

```python
P_i = softmax(S_i)
```

å…¶ä¸­ `S_i = [S_i1, S_i2, ...]`ï¼ˆå…± S_k ä¸ªå…ƒç´ ï¼‰, `P_i = [P_i1, P_i2, ...]`ï¼ˆå…± S_k ä¸ªå…ƒç´ ï¼‰ã€‚

**é—®é¢˜**ï¼šæ±‚ `âˆ‚P_ik / âˆ‚S_ij`ï¼ˆP çš„ç¬¬ k ä¸ªå…ƒç´ å¯¹ S çš„ç¬¬ j ä¸ªå…ƒç´ çš„åå¯¼ï¼‰ã€‚

**æƒ…å†µ 1**ï¼š`k = j`ï¼ˆå¯¹è§’çº¿å…ƒç´ ï¼‰

```python
P_ij = exp(S_ij) / l_i    # å…¶ä¸­ l_i = sum_k(exp(S_ik))

âˆ‚P_ij / âˆ‚S_ij = âˆ‚/âˆ‚S_ij [exp(S_ij) / l_i]
              = [exp(S_ij) * l_i - exp(S_ij) * exp(S_ij)] / l_i^2 
              = exp(S_ij) / l_i * [1 - exp(S_ij) / l_i]
              = P_ij * (1 - P_ij)
```

**æƒ…å†µ 2**ï¼š`k â‰  j`ï¼ˆéå¯¹è§’çº¿å…ƒç´ ï¼‰

```python
P_ik = exp(S_ik) / l_i

âˆ‚P_ik / âˆ‚S_ij = âˆ‚/âˆ‚S_ij [exp(S_ik) / l_i]
              = [0 - exp(S_ik) * exp(S_ij)] / l_i^2    # exp(S_ik) ä¸ä¾èµ– S_ij
              = -exp(S_ik) / l_i * exp(S_ij) / l_i
              = -P_ik * P_ij
```

**æ€»ç»“**ï¼š

```python
âˆ‚P_ik / âˆ‚S_ij = {
    P_ij * (1 - P_ij),  if k = j
    -P_ik * P_ij,       if k â‰  j
}
```

#### C.3 åº”ç”¨é“¾å¼æ³•åˆ™

ç°åœ¨æˆ‘ä»¬è¦è®¡ç®— `dS_ij`ï¼ˆæŸå¤± L å¯¹ S_ij çš„æ¢¯åº¦ï¼‰ã€‚

æ ¹æ®é“¾å¼æ³•åˆ™ï¼š

```python
dS_ij = âˆ‚L/âˆ‚S_ij
      = sum_k( âˆ‚L/âˆ‚P_ik * âˆ‚P_ik/âˆ‚S_ij )    # å¯¹æ‰€æœ‰å— S_ij å½±å“çš„ P_ik æ±‚å’Œ
```

å°† C.2 çš„ç»“æœä»£å…¥ï¼š

```python
dS_ij = dP_ij * P_ij * (1 - P_ij) + sum_{kâ‰ j}( dP_ik * (-P_ik * P_ij) )
      = dP_ij * P_ij * (1 - P_ij) - P_ij * sum_{kâ‰ j}( dP_ik * P_ik )
      = dP_ij * P_ij - dP_ij * P_ij^2 - P_ij * sum_{kâ‰ j}( dP_ik * P_ik )
      = dP_ij * P_ij - P_ij * ( dP_ij * P_ij + sum_{kâ‰ j}( dP_ik * P_ik ) )
      = dP_ij * P_ij - P_ij * sum_k( dP_ik * P_ik )
```

**å®šä¹‰** `delta_i = sum_k( dP_ik * P_ik )`ï¼Œåˆ™ï¼š

```python
dS_ij = P_ij * (dP_ij - delta_i)
```

#### C.4 çŸ©é˜µå½¢å¼

å†™æˆçŸ©é˜µå½¢å¼ï¼š

```python
delta = row_sum(dP âŠ™ P)         # delta_i = sum_k(dP_ik * P_ik)
dS = P âŠ™ (dP - delta[:, None])  # element-wise æ“ä½œ
```

### D. ä¼˜åŒ– `delta` è®¡ç®—æ–¹å¼

å¦‚ C æ‰€ç¤ºï¼šå¦‚æœç›´æ¥æŒ‰ç…§åŸå§‹å®šä¹‰å®ç°ï¼Œå°±æ„å‘³ç€ `delta` éœ€è¦åœ¨ K/V loop ä¸­è®¡ç®—ï¼Œè¿™ä¼šå¯¼è‡´åå¤è®¡ç®—ï¼Œå¢åŠ é¢å¤–è´Ÿæ‹…ã€‚

ç„¶è€Œï¼Œé€šè¿‡æ•°å­¦å˜æ¢å¯ä»¥å‘ç°ï¼Œ`delta` å®é™…ä¸Šåªä¾èµ–äº forward çš„è¾“å‡º `O` ä¸ä¸Šæ¸¸æ¢¯åº¦ `dO`ï¼Œå®Œå…¨å¯ä»¥åœ¨è¿›å…¥ `K/V` loop ä¹‹å‰ï¼Œå•ç‹¬ã€ä¸€æ¬¡æ€§çš„è®¡ç®—å‡º `delta`ã€‚

```python
delta_i = sum_j( dP_ij * P_ij )
    	= sum_j( sum_k(dO_ik * V_jk) * P_ij )    # dP = dO @ V^T
    	= sum_k( dO_ik * sum_j(V_jk * P_ij) )    # äº¤æ¢æ±‚å’Œé¡ºåº
    	= sum_k( dO_ik * sum_j(P_ij * V_jk) )    # ä¹˜æ³•äº¤æ¢å¾‹
   	 	= sum_k( dO_ik * O_ik )                  # O = P @ V
```

å› æ­¤ï¼š

```python
delta = row_sum(dO âŠ™ O)
```

### E. dQ å’Œ dK çš„æ¨å¯¼

#### E.1 åˆ†è§£è®¡ç®—

```python
S = Q @ K^T / sqrt(D)
```

è¿™å¯ä»¥åˆ†è§£ä¸ºä¸¤æ­¥ï¼š

```python
S_temp = Q @ K^T
S = S_temp / sqrt(D)
```

#### E.2 dQ çš„æ¨å¯¼

**æ­¥éª¤ 1**ï¼šè®¡ç®— `dS_temp`

```python
S = S_temp / sqrt(D)
```

æ ¹æ®ç¼©æ”¾çš„æ¢¯åº¦å…¬å¼ï¼š

```python
dS_temp = dS / sqrt(D)
```

**æ­¥éª¤ 2**ï¼šè®¡ç®— `dQ`

```python
S_temp = Q @ K^T
```

æ ¹æ®çŸ©é˜µä¹˜æ³•çš„æ¢¯åº¦å…¬å¼ï¼š

```python
dQ = dS_temp @ K
   = (dS / sqrt(D)) @ K
   = dS @ K / sqrt(D)
```

#### E.3 dK çš„æ¨å¯¼

åŒç†ï¼š

```python
dK = dS_temp^T @ Q
   = (dS / sqrt(D))^T @ Q
   = dS^T @ Q / sqrt(D)
```

### F. å®Œæ•´å…¬å¼æ€»ç»“

```python
# Forward
S = Q @ K^T / sqrt(D)
P = softmax(S)           # row-wise
O = P @ V

# Backward
# æ­¥éª¤ 1ï¼šè®¡ç®— dV
dV = P^T @ dO

# æ­¥éª¤ 2ï¼šè®¡ç®— delta
delta = row_sum(dO âŠ™ O)

# æ­¥éª¤ 3ï¼šè®¡ç®— dS (éœ€è¦é‡å»º P)
dP = dO @ V^T
dS = P âŠ™ (dP - delta[:, None])

# æ­¥éª¤ 4ï¼šè®¡ç®— dQ å’Œ dK
dQ = dS @ K / sqrt(D)
dK = dS^T @ Q / sqrt(D)
```

è¿™å°±æ˜¯æ ‡å‡† Attention çš„å®Œæ•´æ¢¯åº¦å…¬å¼ï¼å¸Œæœ›è¿™ä¸ªè¯¦ç»†çš„æ¨å¯¼èƒ½å¸®åŠ©ä½ ç†è§£ Attention åå‘ä¼ æ’­çš„æ•°å­¦åŸç†ï¼
